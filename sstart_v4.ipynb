{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e541ced7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from skmultiflow.lazy import KNN\n",
    "from skmultiflow.trees import HoeffdingTreeRegressor\n",
    "from skmultiflow.ensemble import AdaptiveRandomForestRegressor\n",
    "import seaborn as sns\n",
    "from datetime import datetime, timedelta\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "58c2e1e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BeijingAirQualityForecaster:\n",
    "    \"\"\"\n",
    "    Stream forecasting system for Beijing Air Quality data.\n",
    "    Predicts air pollutant concentrations (PM2.5, PM10, etc.) based on \n",
    "    previous measurements and meteorological conditions.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, target_pollutant='PM2.5', model_type='knn', \n",
    "                 window_size=168, min_samples_to_predict=24):  # 168 hours = 1 week\n",
    "        \"\"\"\n",
    "        Initialize the forecaster.\n",
    "        \n",
    "        Args:\n",
    "            target_pollutant: Which pollutant to predict ('PM2.5', 'PM10', 'SO2', 'NO2', 'CO', 'O3')\n",
    "            model_type: 'knn', 'hoeffding_tree', or 'adaptive_rf'\n",
    "            window_size: Number of hours to keep in memory (default: 1 week)\n",
    "            min_samples_to_predict: Minimum samples needed before making predictions\n",
    "        \"\"\"\n",
    "        self.target_pollutant = target_pollutant\n",
    "        self.window_size = window_size\n",
    "        self.min_samples_to_predict = min_samples_to_predict\n",
    "        \n",
    "        # Initialize model\n",
    "        self.model = self._initialize_model(model_type)\n",
    "        \n",
    "        # Data preprocessing\n",
    "        self.scaler = StandardScaler()\n",
    "        self.wind_encoder = LabelEncoder()\n",
    "        self.station_encoder = LabelEncoder()\n",
    "        \n",
    "        # Data storage\n",
    "        self.data_buffer = []\n",
    "        self.feature_names = None\n",
    "        self.is_fitted = False\n",
    "        \n",
    "        # Prediction tracking\n",
    "        self.predictions = []\n",
    "        self.actual_values = []\n",
    "        self.prediction_errors = []\n",
    "        self.timestamps = []\n",
    "        \n",
    "        # Performance metrics\n",
    "        self.mae_history = []\n",
    "        self.rmse_history = []\n",
    "        self.r2_history = []\n",
    "        \n",
    "        # Station-specific tracking\n",
    "        self.station_performance = {}\n",
    "        \n",
    "    def _initialize_model(self, model_type):\n",
    "        \"\"\"Initialize the streaming model\"\"\"\n",
    "        if model_type == 'knn':\n",
    "            return KNN(n_neighbors=8, max_window_size=self.window_size, leaf_size=50)\n",
    "        elif model_type == 'hoeffding_tree':\n",
    "            return HoeffdingTreeRegressor(split_criterion='variance_reduction')\n",
    "        elif model_type == 'adaptive_rf':\n",
    "            return AdaptiveRandomForestRegressor(n_estimators=10, random_state=42)\n",
    "        else:\n",
    "            raise ValueError(\"model_type must be 'knn', 'hoeffding_tree', or 'adaptive_rf'\")\n",
    "    \n",
    "    def _prepare_features(self, data_point):\n",
    "        \"\"\"\n",
    "        Prepare features from a single data point.\n",
    "        Expected columns: No, year, month, day, hour, PM2.5, PM10, SO2, NO2, CO, O3, \n",
    "                         TEMP, PRES, DEWP, RAIN, wd, WSPM, station\n",
    "        \"\"\"\n",
    "        features = []\n",
    "        \n",
    "        # Time features\n",
    "        features.extend([\n",
    "            data_point.get('year', 2013),\n",
    "            data_point.get('month', 1),\n",
    "            data_point.get('day', 1),\n",
    "            data_point.get('hour', 0),\n",
    "            data_point.get('hour', 0) % 6,  # 4-hour period of day\n",
    "            1 if data_point.get('hour', 0) >= 6 and data_point.get('hour', 0) <= 18 else 0  # day/night\n",
    "        ])\n",
    "        \n",
    "        # Air quality features (excluding target)\n",
    "        pollutants = ['PM2.5', 'PM10', 'SO2', 'NO2', 'CO', 'O3']\n",
    "        for pollutant in pollutants:\n",
    "            if pollutant != self.target_pollutant:\n",
    "                features.append(data_point.get(pollutant, 0))\n",
    "        \n",
    "        # Meteorological features\n",
    "        features.extend([\n",
    "            data_point.get('TEMP', 0),\n",
    "            data_point.get('PRES', 1013),\n",
    "            data_point.get('DEWP', 0),\n",
    "            data_point.get('RAIN', 0),\n",
    "            data_point.get('WSPM', 0)\n",
    "        ])\n",
    "        \n",
    "        # Wind direction (encoded)\n",
    "        wind_dir = data_point.get('wd', 'N')\n",
    "        if hasattr(self.wind_encoder, 'classes_'):\n",
    "            try:\n",
    "                wind_encoded = self.wind_encoder.transform([wind_dir])[0]\n",
    "            except ValueError:\n",
    "                wind_encoded = 0  # Unknown direction\n",
    "        else:\n",
    "            wind_encoded = 0\n",
    "        features.append(wind_encoded)\n",
    "        \n",
    "        # Station (encoded)\n",
    "        station = data_point.get('station', 'Unknown')\n",
    "        if hasattr(self.station_encoder, 'classes_'):\n",
    "            try:\n",
    "                station_encoded = self.station_encoder.transform([station])[0]\n",
    "            except ValueError:\n",
    "                station_encoded = 0  # Unknown station\n",
    "        else:\n",
    "            station_encoded = 0\n",
    "        features.append(station_encoded)\n",
    "        \n",
    "        return np.array(features)\n",
    "    \n",
    "    def _get_lagged_features(self, n_lags=5):\n",
    "        \"\"\"Get lagged features from recent data\"\"\"\n",
    "        if len(self.data_buffer) < n_lags:\n",
    "            return np.array([])\n",
    "        \n",
    "        lagged_values = []\n",
    "        for i in range(1, n_lags + 1):\n",
    "            if len(self.data_buffer) >= i:\n",
    "                lagged_values.append(self.data_buffer[-i].get(self.target_pollutant, 0))\n",
    "            else:\n",
    "                lagged_values.append(0)\n",
    "        \n",
    "        return np.array(lagged_values)\n",
    "    \n",
    "    def process_new_point(self, data_point):\n",
    "        \"\"\"\n",
    "        Process a new air quality data point.\n",
    "        \n",
    "        Args:\n",
    "            data_point: Dictionary with air quality measurements\n",
    "                       Expected keys: year, month, day, hour, PM2.5, PM10, SO2, NO2, \n",
    "                                    CO, O3, TEMP, PRES, DEWP, RAIN, wd, WSPM, station\n",
    "        \"\"\"\n",
    "        current_time = datetime(\n",
    "            data_point.get('year', 2013),\n",
    "            data_point.get('month', 1),\n",
    "            data_point.get('day', 1),\n",
    "            data_point.get('hour', 0)\n",
    "        )\n",
    "        \n",
    "        target_value = data_point.get(self.target_pollutant, 0)\n",
    "        station = data_point.get('station', 'Unknown')\n",
    "        \n",
    "        # Handle missing target values\n",
    "        if pd.isna(target_value) or target_value < 0:\n",
    "            print(f\"Skipping point with invalid {self.target_pollutant} value: {target_value}\")\n",
    "            return\n",
    "        \n",
    "        # Step 1: Make prediction if we have enough data\n",
    "        if len(self.data_buffer) >= self.min_samples_to_predict:\n",
    "            try:\n",
    "                # Prepare features\n",
    "                current_features = self._prepare_features(data_point)\n",
    "                lagged_features = self._get_lagged_features(n_lags=5)\n",
    "                \n",
    "                if len(lagged_features) > 0:\n",
    "                    full_features = np.concatenate([current_features, lagged_features])\n",
    "                else:\n",
    "                    full_features = current_features\n",
    "                \n",
    "                # Prepare training data from buffer\n",
    "                if not self.is_fitted:\n",
    "                    self._fit_initial_model()\n",
    "                \n",
    "                # Make prediction\n",
    "                if hasattr(self.model, 'predict') and self.is_fitted:\n",
    "                    prediction = self.model.predict(full_features.reshape(1, -1))[0]\n",
    "                    \n",
    "                    # Calculate error\n",
    "                    error = abs(prediction - target_value)\n",
    "                    relative_error = error / max(target_value, 1.0) * 100\n",
    "                    \n",
    "                    # Store prediction results\n",
    "                    self.predictions.append(prediction)\n",
    "                    self.actual_values.append(target_value)\n",
    "                    self.prediction_errors.append(error)\n",
    "                    self.timestamps.append(current_time)\n",
    "                    \n",
    "                    # Update performance metrics\n",
    "                    if len(self.predictions) >= 2:\n",
    "                        mae = mean_absolute_error(self.actual_values, self.predictions)\n",
    "                        rmse = np.sqrt(mean_squared_error(self.actual_values, self.predictions))\n",
    "                        r2 = r2_score(self.actual_values, self.predictions) if len(set(self.actual_values)) > 1 else 0\n",
    "                        \n",
    "                        self.mae_history.append(mae)\n",
    "                        self.rmse_history.append(rmse)\n",
    "                        self.r2_history.append(r2)\n",
    "                    \n",
    "                    # Update station-specific performance\n",
    "                    if station not in self.station_performance:\n",
    "                        self.station_performance[station] = {'errors': [], 'predictions': [], 'actual': []}\n",
    "                    \n",
    "                    self.station_performance[station]['errors'].append(error)\n",
    "                    self.station_performance[station]['predictions'].append(prediction)\n",
    "                    self.station_performance[station]['actual'].append(target_value)\n",
    "                    \n",
    "                    print(f\"{current_time.strftime('%Y-%m-%d %H:%00')} | Station: {station} | \"\n",
    "                          f\"Predicted: {prediction:.1f} | Actual: {target_value:.1f} | \"\n",
    "                          f\"Error: {error:.1f} ({relative_error:.1f}%)\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Prediction error: {e}\")\n",
    "        \n",
    "        # Step 2: Update buffer with new point\n",
    "        self.data_buffer.append(data_point)\n",
    "        \n",
    "        # Maintain sliding window\n",
    "        if len(self.data_buffer) > self.window_size:\n",
    "            self.data_buffer.pop(0)\n",
    "        \n",
    "        # Step 3: Update model incrementally\n",
    "        if self.is_fitted and len(self.data_buffer) > self.min_samples_to_predict:\n",
    "            try:\n",
    "                current_features = self._prepare_features(data_point)\n",
    "                lagged_features = self._get_lagged_features(n_lags=5)\n",
    "                \n",
    "                if len(lagged_features) > 0:\n",
    "                    full_features = np.concatenate([current_features, lagged_features])\n",
    "                    \n",
    "                    # Scale features\n",
    "                    if hasattr(self.scaler, 'mean_'):\n",
    "                        scaled_features = self.scaler.transform(full_features.reshape(1, -1))\n",
    "                    else:\n",
    "                        scaled_features = full_features.reshape(1, -1)\n",
    "                    \n",
    "                    # Incremental learning\n",
    "                    self.model.partial_fit(scaled_features, [target_value])\n",
    "            except Exception as e:\n",
    "                print(f\"Model update error: {e}\")\n",
    "    \n",
    "    def _fit_initial_model(self):\n",
    "        \"\"\"Fit the initial model using buffered data\"\"\"\n",
    "        if len(self.data_buffer) < self.min_samples_to_predict:\n",
    "            return\n",
    "        \n",
    "        try:\n",
    "            # Prepare training data\n",
    "            X_train = []\n",
    "            y_train = []\n",
    "            \n",
    "            for i, point in enumerate(self.data_buffer[5:]):  # Skip first 5 for lag features\n",
    "                features = self._prepare_features(point)\n",
    "                \n",
    "                # Get lagged features from previous points\n",
    "                lagged_values = []\n",
    "                for lag in range(1, 6):  # 5 lags\n",
    "                    if i + 5 - lag >= 0:\n",
    "                        lagged_values.append(self.data_buffer[i + 5 - lag].get(self.target_pollutant, 0))\n",
    "                    else:\n",
    "                        lagged_values.append(0)\n",
    "                \n",
    "                full_features = np.concatenate([features, np.array(lagged_values)])\n",
    "                target_value = point.get(self.target_pollutant, 0)\n",
    "                \n",
    "                if not pd.isna(target_value) and target_value >= 0:\n",
    "                    X_train.append(full_features)\n",
    "                    y_train.append(target_value)\n",
    "            \n",
    "            if len(X_train) > 0:\n",
    "                X_train = np.array(X_train)\n",
    "                y_train = np.array(y_train)\n",
    "                \n",
    "                # Fit encoders\n",
    "                wind_dirs = [point.get('wd', 'N') for point in self.data_buffer]\n",
    "                stations = [point.get('station', 'Unknown') for point in self.data_buffer]\n",
    "                \n",
    "                self.wind_encoder.fit(wind_dirs)\n",
    "                self.station_encoder.fit(stations)\n",
    "                \n",
    "                # Scale features\n",
    "                self.scaler.fit(X_train)\n",
    "                X_train_scaled = self.scaler.transform(X_train)\n",
    "                \n",
    "                # Train model\n",
    "                self.model.partial_fit(X_train_scaled, y_train)\n",
    "                self.is_fitted = True\n",
    "                \n",
    "                print(f\"Initial model fitted with {len(X_train)} samples\")\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Initial model fitting error: {e}\")\n",
    "    \n",
    "    def get_performance_summary(self):\n",
    "        \"\"\"Get overall performance summary\"\"\"\n",
    "        if len(self.predictions) < 2:\n",
    "            return \"Not enough predictions for performance summary\"\n",
    "        \n",
    "        mae = mean_absolute_error(self.actual_values, self.predictions)\n",
    "        rmse = np.sqrt(mean_squared_error(self.actual_values, self.predictions))\n",
    "        r2 = r2_score(self.actual_values, self.predictions) if len(set(self.actual_values)) > 1 else 0\n",
    "        \n",
    "        summary = f\"\"\"\n",
    "                    === Performance Summary for {self.target_pollutant} Forecasting ===\n",
    "                    Total Predictions: {len(self.predictions)}\n",
    "                    Mean Absolute Error (MAE): {mae:.2f} µg/m³\n",
    "                    Root Mean Square Error (RMSE): {rmse:.2f} µg/m³\n",
    "                    R² Score: {r2:.3f}\n",
    "                    Average Error: {np.mean(self.prediction_errors):.2f} µg/m³\n",
    "                    Median Error: {np.median(self.prediction_errors):.2f} µg/m³\n",
    "\n",
    "                    Station-specific Performance:\"\"\"\n",
    "        \n",
    "        for station, perf in self.station_performance.items():\n",
    "            if len(perf['errors']) > 0:\n",
    "                station_mae = np.mean(perf['errors'])\n",
    "                summary += f\"\\n  {station}: MAE = {station_mae:.2f} µg/m³ ({len(perf['errors'])} predictions)\"\n",
    "        \n",
    "        return summary\n",
    "    \n",
    "    def plot_results(self, last_n_hours=168):  # Last week by default\n",
    "        \"\"\"Plot forecasting results\"\"\"\n",
    "        if len(self.predictions) < 2:\n",
    "            print(\"Not enough predictions to plot\")\n",
    "            return\n",
    "        \n",
    "        # Limit to last N hours\n",
    "        start_idx = max(0, len(self.predictions) - last_n_hours)\n",
    "        recent_times = self.timestamps[start_idx:]\n",
    "        recent_predictions = self.predictions[start_idx:]\n",
    "        recent_actual = self.actual_values[start_idx:]\n",
    "        recent_errors = self.prediction_errors[start_idx:]\n",
    "        \n",
    "        fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(16, 12))\n",
    "        \n",
    "        # Plot 1: Time series comparison\n",
    "        ax1.plot(recent_times, recent_actual, 'b-', label='Actual', alpha=0.8, linewidth=1.5)\n",
    "        ax1.plot(recent_times, recent_predictions, 'r--', label='Predicted', alpha=0.8, linewidth=1.5)\n",
    "        ax1.set_title(f'{self.target_pollutant} Forecasting: Actual vs Predicted')\n",
    "        ax1.set_xlabel('Time')\n",
    "        ax1.set_ylabel(f'{self.target_pollutant} Concentration (µg/m³)')\n",
    "        ax1.legend()\n",
    "        ax1.grid(True, alpha=0.3)\n",
    "        ax1.tick_params(axis='x', rotation=45)\n",
    "        \n",
    "        # Plot 2: Prediction errors over time\n",
    "        ax2.plot(recent_times, recent_errors, 'g-', alpha=0.7)\n",
    "        ax2.axhline(y=np.mean(recent_errors), color='red', linestyle='--', \n",
    "                   label=f'Mean Error: {np.mean(recent_errors):.1f}')\n",
    "        ax2.set_title('Prediction Errors Over Time')\n",
    "        ax2.set_xlabel('Time')\n",
    "        ax2.set_ylabel('Absolute Error (µg/m³)')\n",
    "        ax2.legend()\n",
    "        ax2.grid(True, alpha=0.3)\n",
    "        ax2.tick_params(axis='x', rotation=45)\n",
    "        \n",
    "        # Plot 3: Scatter plot\n",
    "        ax3.scatter(recent_actual, recent_predictions, alpha=0.6, s=20)\n",
    "        min_val = min(min(recent_actual), min(recent_predictions))\n",
    "        max_val = max(max(recent_actual), max(recent_predictions))\n",
    "        ax3.plot([min_val, max_val], [min_val, max_val], 'r--', alpha=0.8)\n",
    "        ax3.set_xlabel(f'Actual {self.target_pollutant} (µg/m³)')\n",
    "        ax3.set_ylabel(f'Predicted {self.target_pollutant} (µg/m³)')\n",
    "        ax3.set_title('Predicted vs Actual Values')\n",
    "        ax3.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Plot 4: Performance metrics over time\n",
    "        if len(self.mae_history) > 1:\n",
    "            recent_mae = self.mae_history[start_idx:] if len(self.mae_history) > start_idx else self.mae_history\n",
    "            recent_r2 = self.r2_history[start_idx:] if len(self.r2_history) > start_idx else self.r2_history\n",
    "            \n",
    "            ax4_twin = ax4.twinx()\n",
    "            line1 = ax4.plot(range(len(recent_mae)), recent_mae, 'purple', label='MAE')\n",
    "            line2 = ax4_twin.plot(range(len(recent_r2)), recent_r2, 'orange', label='R²')\n",
    "            \n",
    "            ax4.set_xlabel('Time Period')\n",
    "            ax4.set_ylabel('MAE (µg/m³)', color='purple')\n",
    "            ax4_twin.set_ylabel('R² Score', color='orange')\n",
    "            ax4.set_title('Performance Metrics Over Time')\n",
    "            \n",
    "            lines = line1 + line2\n",
    "            labels = [l.get_label() for l in lines]\n",
    "            ax4.legend(lines, labels, loc='upper left')\n",
    "            ax4.grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eb8b539",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage and data loading functions\n",
    "def load_beijing_data(file_path):\n",
    "    \"\"\"Load Beijing Air Quality dataset\"\"\"\n",
    "    try:\n",
    "        df = pd.read_csv(file_path)\n",
    "        \n",
    "        # Clean column names\n",
    "        df.columns = df.columns.str.strip()\n",
    "        \n",
    "        # Handle missing values\n",
    "        numeric_columns = ['PM2.5', 'PM10', 'SO2', 'NO2', 'CO', 'O3', 'TEMP', 'PRES', 'DEWP', 'RAIN', 'WSPM']\n",
    "        for col in numeric_columns:\n",
    "            if col in df.columns:\n",
    "                df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "        \n",
    "        # Sort by time\n",
    "        df = df.sort_values(['station', 'year', 'month', 'day', 'hour']).reset_index(drop=True)\n",
    "        \n",
    "        print(f\"Loaded {len(df)} records from {df['station'].nunique()} stations\")\n",
    "        print(f\"Time range: {df['year'].min()}-{df['month'].min():02d} to {df['year'].max()}-{df['month'].max():02d}\")\n",
    "        print(f\"Available pollutants: {[col for col in numeric_columns if col in df.columns]}\")\n",
    "        \n",
    "        return df\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error loading data: {e}\")\n",
    "        return None\n",
    "\n",
    "def simulate_streaming_data(df, forecaster, start_idx=0, n_points=1000):\n",
    "    \"\"\"Simulate streaming data arrival\"\"\"\n",
    "    print(f\"Starting streaming simulation with {n_points} points...\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    end_idx = min(start_idx + n_points, len(df))\n",
    "    \n",
    "    for i in range(start_idx, end_idx):\n",
    "        data_point = df.iloc[i].to_dict()\n",
    "        forecaster.process_new_point(data_point)\n",
    "        \n",
    "        # Print summary every 24 hours\n",
    "        if (i - start_idx + 1) % 24 == 0:\n",
    "            hours_processed = i - start_idx + 1\n",
    "            if len(forecaster.predictions) > 0:\n",
    "                recent_mae = np.mean(forecaster.prediction_errors[-24:]) if len(forecaster.prediction_errors) >= 24 else np.mean(forecaster.prediction_errors)\n",
    "                print(f\"\\n--- After {hours_processed} hours ---\")\n",
    "                print(f\"Recent 24h Average Error: {recent_mae:.2f} µg/m³\")\n",
    "    \n",
    "    return forecaster\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9399abd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Initialize forecaster for PM2.5 prediction\n",
    "    forecaster = BeijingAirQualityForecaster(\n",
    "        target_pollutant='PM2.5',\n",
    "        model_type='knn',  # or 'hoeffding_tree', 'adaptive_rf'\n",
    "        window_size=168,   # 1 week of hourly data\n",
    "        min_samples_to_predict=24  # 1 day minimum\n",
    "    )\n",
    "    \n",
    "    # To use with real data, uncomment these lines:\n",
    "    # df = load_beijing_data('path/to/your/beijing_air_quality.csv')\n",
    "    # if df is not None:\n",
    "    #     forecaster = simulate_streaming_data(df, forecaster, start_idx=1000, n_points=500)\n",
    "    #     print(forecaster.get_performance_summary())\n",
    "    #     forecaster.plot_results(last_n_hours=168)\n",
    "    \n",
    "    print(\"Beijing Air Quality Forecaster initialized!\")\n",
    "    print(\"To use with your data:\")\n",
    "    print(\"1. df = load_beijing_data('your_file.csv')\")\n",
    "    print(\"2. forecaster = simulate_streaming_data(df, forecaster, start_idx=1000, n_points=500)\")\n",
    "    print(\"3. forecaster.plot_results()\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "549bbd7f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (multiflow_old)",
   "language": "python",
   "name": "multiflow_old"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
